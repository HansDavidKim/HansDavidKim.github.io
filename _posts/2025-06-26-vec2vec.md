---
layout: post
title: "논문 리뷰 : Harnessing the Universal Geometry of Embeddings"
categories: paper-review
tags: [paper-review]
math: true
---

## 1. Introduction
---
해당 논문은 Vec2Vec 알고리즘을 통하여 Platonic Representation Hypothesis 이 실증적으로 참임을 보여주며, 이를 통해 Vector DB 에서의 내용을 Semantic-level 에서 번역함으로서 보안에 문제가 될 수 있음을 제시한다.

#### Platonic Representation Hypothesis :
>Deep Learning 모델들이 다양한 데이터를 학습하며 이들이 학습한 표현은 서로 수렴하며, 이는 플라톤이 말하는 '이데아'와 같은 현실의 공통 구조로 향해 간다는 가설

기존의 Platonic Representation Hypothesis 는 vision model 에 국한된 내용이었으나, Cornell 대학교 연구팀은 위의 가설이 언어 모델에서도 성립함을 실험을 통해 실증적으로 보여주었다.

## 2. Problem Formulation : Unsupervised Embedding Translation
---
### Limitation of correspondence methods
기존의 다른 tokenizer 간의 임베딩을 대응시키는 방식은 서로 다른 임베딩 공간 정렬을 위해 입력 간에 강한 유사성을 전제로 하기에, 입력이나 tokenizer가 다를 경우 '의미' 정렬이 불가능하다는 근본적 한계 지점을 지닌다.

이를 해결하고자 논문에서는 <strong>Platonic Representation Hypothesis</strong>가 참이라 가정하여, 비지도 학습 방식으로 알려지지 않은 임베딩 공간의 벡터들을 알려진 임베딩 공간의 벡터들로 변환함으로서 의미적 정렬 및 번역을 수행한다.

<img src="https://miro.medium.com/v2/resize:fit:2000/1*EsmLEwXg0sp98F7PBKeOMw.png">

## 3. vec2vec
---
CV (Computer Vision) 분야에서, <strong>cycle consistency</strong>와 <strong>adversarial regularization</strong>을 통해 비지도 번역을 성공적으로 수행할 수 있다.

본 논문에서는 이 방식을 언어 모델로 확장하여 unsupervised embedding translation 태스크를 수행한다.
### 3.1 Architecture
1. <strong>입력 어댑터 (Input Adapter)</strong> :   각 어댑터는 각각의 임베딩 벡터를 universal latent representation의 차원의 벡터로 변환 시키는 역할을 수행한다.  
$A_1 : R^d \rightarrow R^z$   
$A_2 : R^d \rightarrow R^z$    

2. <strong>공유 백본 (Shared Backbone Network)</strong> : Adapted input에서 common latent embedding을 추출해낸다.  
$T : R^z\rightarrow R^z$

3. <strong>출력 어댑터 (Output Adapter)</strong> : common latent embedding을 다시 각 tokenizer에 맞는 embedding space 로 복원  
$B_1 : R^z \rightarrow R^d$  
$B_2 : R^z \rightarrow R^d$

위의 세 요소를 통해 Translation function $F_1,\,F_2$과 Reconstruction mapping $R_1,\,R_2$에 대해 다음과 같이 나타낼 수 있다.
> $F_1 = B_2 \circ T \circ A_1$  
$F_2 = B_1 \circ T \circ A_2$  
$R_1 = B_1 \circ T \circ A_1$  
$R_2 = B_2 \circ T \circ A_2$

이미지에서와는 달리, 임베딩은 spatial bias는 가지지 않는다. 그렇기에 기존의 방식인 CNN이 아닌 Residual Connection, MLP, layer normalization, SiLU 활성 함수를 통해 각 아키텍쳐를 구현한다.

Discriminator는 단순화를 위해 residual connection을 제거하고 위와 동일한 구조를 채택한다.

### 3.2 Optimization
---
앞으로 다룰 내용을 명료하게 이해하기 위해, GAN Loss에 대해 정의 및 기술하도록 하자.

GAN 기법은 기본적으로 generator model과 discriminator model 두 개를 활용하여 학습시키는 기법으로, generator는 discriminator를 더 "잘 속이도록", discriminator는 반대로 "더 잘 판별하도록" 경쟁적으로 학습하는 방식이다.

이를 반영한 Loss function은 아래와 같다.

#### GAN Loss / Minimax Loss
> $$\min_{G}\max_{D}V(D,G)=\mathbb{E}_{x\sim p_{data}}[\log D(x)]\,+\,\mathbb{E}_{z\sim p_z(z)}[1-D(G(z))]$$

각 항들을 해석해보면 실제 데이터 x를 참이라 판정할 확률에 대한 로그 값에 대한 기댓값과 생성된 데이터 z에 대해 거짓이라고 판정할 확률에 대한 로그 값에 대한 기댓값이다.

이를 최대화 하려는 Discriminator (판별자)와, 그걸 최소화 하고자 하는 Generator (생성자) 의 경쟁을 통해 GAN 방식은 생성자의 성능을 극대화 하고자 하는 프레임워크다.

다시 Vec2Vec으로 돌아와, 위의 GAN 방식을 적용해보자.  
각 architecture component의 parameters를 다음과 같이 표현하자.

$$\theta=\{A_1,A_2,T,B_1,B_2\}$$
<br>
vec2vec 최적화 문제를 다음과 같이 표현할 수 있다.

> $$\theta^*= \operatorname*{argmin}_{\theta}\max_{D_1,D_2, D_1^l,D_2^l}\mathcal{L}_{adv}(F_1,F_2,D_1,D_2,D_1^l,D_2^l)+\lambda_{gen}\mathcal{L}_{gen}(\theta)$$

지금 이 식만으로는 Loss function에 대해 바로 이해하기 어렵기에, 위의 loss를 앞서 다룬 GAN Loss를 활용해 분리하도록 하자.

#### 1. Adversarial Loss
> $$\mathcal{L}_{adv}(F_1,F_2,D_1,D_2,D_1^l,D_2^l)=\mathcal{L}_{GAN}(D_1,F_1)+\mathcal{L}_{GAN}(D_2,F_2)+$$  
>$$ \mathcal{L}_{GAN}(D_1^l,T\circ A_1)+\mathcal{L}_{GAN}(D_2^l, T\circ A_2)$$ 

여기서 질문, $T\circ A_1$과 $T\circ A_2$ 둘 모두 common latent embedding에 해당 되기에 ground truth가 존재하지 않을 터인데 어떻게 학습이 가능한 것인가?

이는 다른 encoder가 생성한 common latent embedding 을 ground truth 라고 가정하여 학습하는 것으로, 두 common latent embedding 서로 간의 구분을 할 수 없는 방향으로 학습이 이루어지게 된다.

#### 2. Generator Loss
위의 Adversarial Loss는 모두 GAN Loss로 분리될 수 있었으나, Generator Loss는 세 가지의 다른 Loss function으로 분리된다.

#### Reconstruction Loss
Common Latent Embedding에서 원래의 embedding 으로 복원 될 수 있도록 조정한다.

> $$\mathcal{L}_{rec}(R_1,R_2)=\mathbb{E}_{x\sim p}\left\| R_1(x) - x \right\|_2^2+\mathbb{E}_{y\sim q}\left\| R_2(y) - y \right\|_2^2$$
>$$p\,and\,q\,are\,distributions\,sampled\,from\,M_1\,and\,M_2$$

#### Cycle-Consistency
지도 학습 방식의 pair alignment(임베딩 간 1:1 대응)을 직접 학습할 수 없는 경우, 이를 우회적으로 근사하기 위한 비지도 학습 기반의 목적 함수.
>$$
\mathcal{L}_{\text{CC}}(F_1, F_2) = \mathbb{E}_{x \sim p} \left\| F_2(F_1(x)) - x \right\|_2^2 + \mathbb{E}_{y \sim q} \left\| F_1(F_2(y)) - y \right\|_2^2
$$

쉽게 말해 다른 embedding space에서 다시 original embedding space로 변환 되더라도 original embedding이 유지되도록 강제하는 역할을 수행한다.

#### Vector Space Preservation
두 embedding 간의 기하학적 구조가 유지 되면서 번역이 되도록 강제한다.  
(embedding 간의 내적 등의 기하학적 성질이 다른 embedding space에서도 동일할 것이라는 Strong Platonic Assumption을 가정하는 loss function)
>$$\mathcal{L}_{\text{VSP}} = \sum_{i, j} \left( \langle x_i, x_j \rangle - \langle F(x_i), F(x_j) \rangle \right)^2
$$

이를 결합하여 Generator Loss function을 구할 수 있고, 각 loss에 대한 중요도를 계수(coefficient)에 반영하여 아래와 같이 표현한다.

>$$\mathcal{L}_{\text{gen}}(\theta) = \lambda_{\text{rec}} \, \mathcal{L}_{\text{rec}}(R_1, R_2) + \lambda_{\text{CC}} \, \mathcal{L}_{\text{CC}}(F_1, F_2) + \lambda_{\text{VSP}} \, \mathcal{L}_{\text{VSP}}(F_1, F_2)
$$

## 4. 감상
---
2학년 2학기를 마친 겨울 방학에 Stanford Online에서 ML Course를 들으며 배웠던 개념인 Double-Descent는 결국 over-parametrization을 통한 implicit regularization을 통해 test loss를 줄여나가는 방식이었다.  

이는 곧 extrinsic dimension보다 intrinsic dimension이 훨씬 작기에 발생하는 현상이었는데, Strong Platonic Assumption을 보며 이 또한 tokenizer 간에서도 over-parametrization이 발생했고 이에 따라 유사한 intrinsic dimension을 가지게 되지 않을까 하는 생각이 들었다.

그렇기에 충분히 잘 학습된 토크나이저라면 절대적 위치만 다를 뿐, 각도와 벡터 간의 상대적 거리 등은 결국엔 동일할 수 밖에 없을 것이란 생각이 들어 직관과도 잘 부합하며 이를 실증적으로 보여준 "잘 작성된" 논문이라 느꼈다.