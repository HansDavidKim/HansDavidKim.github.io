---
layout: post
title: "논문 리뷰 : Harnessing the Universal Geometry of Embeddings"
categories: paper-review
tags: [paper-review]
math: true
---

## 1. Introduction
---
해당 논문은 Vec2Vec 알고리즘을 통하여 Platonic Representation Hypothesis 이 실증적으로 참임을 보여주며, 이를 통해 Vector DB 에서의 내용을 Semantic-level 에서 번역함으로서 보안에 문제가 될 수 있음을 제시한다.

#### Platonic Representation Hypothesis :
>Deep Learning 모델들이 다양한 데이터를 학습하며 이들이 학습한 표현은 서로 수렴하며, 이는 플라톤이 말하는 '이데아'와 같은 현실의 공통 구조로 향해 간다는 가설

기존의 Platonic Representation Hypothesis 는 vision model 에 국한된 내용이었으나, Cornell 대학교 연구팀은 위의 가설이 언어 모델에서도 성립함을 실험을 통해 실증적으로 보여주었다.

## 2. Problem Formulation : Unsupervised Embedding Translation
---
### Limitation of correspondence methods
기존의 다른 tokenizer 간의 임베딩을 대응시키는 방식은 서로 다른 임베딩 공간 정렬을 위해 입력 간에 강한 유사성을 전제로 하기에, 입력이나 tokenizer가 다를 경우 '의미' 정렬이 불가능하다는 근본적 한계 지점을 지닌다.

이를 해결하고자 논문에서는 <strong>Platonic Representation Hypothesis</strong>가 참이라 가정하여, 비지도 학습 방식으로 알려지지 않은 임베딩 공간의 벡터들을 알려진 임베딩 공간의 벡터들로 변환함으로서 의미적 정렬 및 번역을 수행한다.

<img src="https://miro.medium.com/v2/resize:fit:2000/1*EsmLEwXg0sp98F7PBKeOMw.png">

## 3. vec2vec
---
CV (Computer Vision) 분야에서, <strong>cycle consistency</strong>와 <strong>adversarial regularization</strong>을 통해 비지도 번역을 성공적으로 수행할 수 있다.

본 논문에서는 이 방식을 언어 모델로 확장하여 unsupervised embedding translation 태스크를 수행한다.
### 3.1 Architecture
1. <strong>입력 어댑터 (Input Adapter)</strong> :   각 어댑터는 각각의 임베딩 벡터를 universal latent representation의 차원의 벡터로 변환 시키는 역할을 수행한다.  
$A_1 : R^d \rightarrow R^z$   
$A_2 : R^d \rightarrow R^z$    

2. <strong>공유 백본 (Shared Backbone Network)</strong> : Adapted input에서 common latent embedding을 추출해낸다.  
$T : R^z\rightarrow R^z$

3. <strong>출력 어댑터 (Output Adapter)</strong> : common latent embedding을 다시 각 tokenizer에 맞는 embedding space 로 복원  
$B_1 : R^z \rightarrow R^d$  
$B_2 : R^z \rightarrow R^d$

위의 세 요소를 통해 Translation function $F_1,\,F_2$과 Reconstruction mapping $R_1,\,R_2$에 대해 다음과 같이 나타낼 수 있다.
> $F_1 = B_2 \circ T \circ A_1$  
$F_2 = B_1 \circ T \circ A_2$  
$R_1 = B_1 \circ T \circ A_1$  
$R_2 = B_2 \circ T \circ A_2$

이미지에서와는 달리, 임베딩은 spatial bias는 가지지 않는다. 그렇기에 기존의 방식인 CNN이 아닌 Residual Connection, MLP, layer normalization, SiLU 활성 함수를 통해 각 아키텍쳐를 구현한다.

Discriminator는 단순화를 위해 residual connection을 제거하고 위와 동일한 구조를 채택한다.